{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dattnguyen1991/aio_warmup_draft/blob/main/warmup/w3_streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znxPoNGjvj-D"
      },
      "source": [
        "##**Install Libs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqnpspqpbcIV",
        "outputId": "e8a3ce4a-49e7-498c-c6d8-e6b494bc0b1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit==1.41.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gX27RPk8oH23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Setup Exposed env**"
      ],
      "metadata": {
        "id": "97RN9ylIoU73"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG3k13yo2S4K",
        "outputId": "a21a25a4-15b3-4a0e-9da3-648e1c8aa15a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test-exposed-server.py\n"
          ]
        }
      ],
      "source": [
        "# Create simple streamlit project to test exposed env\n",
        "%%writefile test-exposed-server.py\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"My project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First approach**  \n",
        "Using localTunnel: https://github.com/localtunnel/localtunnel"
      ],
      "metadata": {
        "id": "yiNu6sLKpGNC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LsBub0EfvtTy"
      },
      "outputs": [],
      "source": [
        "# Get localTunnel's password (Using wget or curl)\n",
        "!wget -q -O - https://loca.lt/mytunnelpassword\n",
        "# !curl https://loca.lt/mytunnelpassword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxeNVvel2QHC"
      },
      "outputs": [],
      "source": [
        "# Run and expose streamlit port 8501\n",
        "!streamlit run test-exposed-server.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second approach**  \n",
        "Using ngrok: https://ngrok.com/"
      ],
      "metadata": {
        "id": "XFgiPB6upW_-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMsXBx1KLlpW",
        "outputId": "a82978d7-6df3-4a22-c037-cd0d0af57689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0KAuthtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['NGROK_TOKEN'] = userdata.get('NGROK_TOKEN')\n",
        "\n",
        "# Authenticate ngrok with token getting from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "!npx ngrok config add-authtoken $NGROK_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run and expose streamlit port 8501\n",
        "!streamlit run test-exposed-server.py & npx ngrok http 8501 --log=stdout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Rfj_yHq4ohOx",
        "outputId": "13ca8f70-58aa-4945-df41-8f16017360bc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.245.135.162:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\u001b[32mINFO\u001b[0m[01-04|14:22:17] no configuration paths supplied \n",
            "\u001b[32mINFO\u001b[0m[01-04|14:22:17] using configuration at default config path \u001b[32mpath\u001b[0m=/root/.config/ngrok/ngrok.yml\n",
            "\u001b[32mINFO\u001b[0m[01-04|14:22:17] open config file                         \u001b[32mpath\u001b[0m=/root/.config/ngrok/ngrok.yml \u001b[32merr\u001b[0m=nil\n",
            "t=2025-01-04T14:22:17+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "t=2025-01-04T14:22:17+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "t=2025-01-04T14:22:17+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "t=2025-01-04T14:22:17+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=command_line addr=http://localhost:8501 url=https://c80d-35-245-135-162.ngrok-free.app\n",
            "t=2025-01-04T14:22:21+0000 lvl=info msg=\"join connections\" obj=join id=f251a7688f34 l=127.0.0.1:8501 r=115.75.32.98:64537\n",
            "t=2025-01-04T14:22:21+0000 lvl=info msg=\"join connections\" obj=join id=fc06e0eeae08 l=127.0.0.1:8501 r=115.75.32.98:64537\n",
            "t=2025-01-04T14:22:21+0000 lvl=info msg=\"join connections\" obj=join id=eed07bfb7878 l=127.0.0.1:8501 r=115.75.32.98:64537\n",
            "t=2025-01-04T14:22:21+0000 lvl=info msg=\"join connections\" obj=join id=0db9dfce4f85 l=127.0.0.1:8501 r=115.75.32.98:64537\n",
            "t=2025-01-04T14:22:21+0000 lvl=info msg=\"join connections\" obj=join id=68575bbda3f6 l=127.0.0.1:8501 r=115.75.32.98:64537\n",
            "t=2025-01-04T14:22:34+0000 lvl=info msg=\"join connections\" obj=join id=279bedd82b6d l=127.0.0.1:8501 r=115.75.32.98:64998\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "t=2025-01-04T14:23:53+0000 lvl=info msg=\"received stop request\" obj=app stopReq=\"{err:<nil> restart:false}\"\n",
            "t=2025-01-04T14:23:53+0000 lvl=info msg=\"session closing\" obj=tunnels.session err=nil\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIphK8ge2T0L"
      },
      "source": [
        "## **Basic**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykNL9LHm6yam",
        "outputId": "c2070356-1d57-4e6c-c2d1-985e679f6d2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Wusjqpr7xmSl5muHPkyBTpOCVtJI-avQ\n",
            "To: /content/Theme.jpeg\n",
            "\r  0% 0.00/161k [00:00<?, ?B/s]\r100% 161k/161k [00:00<00:00, 21.5MB/s]\n",
            "--2025-01-04 14:28:43--  https://picsum.photos/id/237/200/300\n",
            "Resolving picsum.photos (picsum.photos)... 104.26.4.30, 104.26.5.30, 172.67.74.163, ...\n",
            "Connecting to picsum.photos (picsum.photos)|104.26.4.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://fastly.picsum.photos/id/237/200/300.jpg?hmac=TmmQSbShHz9CdQm0NkEjx1Dyh_Y984R9LpNrpvH2D_U [following]\n",
            "--2025-01-04 14:28:44--  https://fastly.picsum.photos/id/237/200/300.jpg?hmac=TmmQSbShHz9CdQm0NkEjx1Dyh_Y984R9LpNrpvH2D_U\n",
            "Resolving fastly.picsum.photos (fastly.picsum.photos)... 151.101.1.91, 151.101.65.91, 151.101.129.91, ...\n",
            "Connecting to fastly.picsum.photos (fastly.picsum.photos)|151.101.1.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10839 (11K) [image/jpeg]\n",
            "Saving to: ‘image.jpg’\n",
            "\n",
            "image.jpg           100%[===================>]  10.58K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-04 14:28:44 (68.2 MB/s) - ‘image.jpg’ saved [10839/10839]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download image\n",
        "!gdown 1Wusjqpr7xmSl5muHPkyBTpOCVtJI-avQ\n",
        "!wget -O image.jpg https://picsum.photos/id/237/200/300\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pVG-MH5zz6i",
        "outputId": "a768ecca-165e-4180-aa94-868ec3762fdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit-components.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile streamlit-components.py\n",
        "import streamlit as st\n",
        "\n",
        "# Text elements\n",
        "st.title(\"MY PROJECT\")\n",
        "st.header(\"Header\")\n",
        "st.subheader(\"Subheader\")\n",
        "st.caption(\"Project for testing streamlit\")\n",
        "st.text(\"Testing element\")\n",
        "\n",
        "st.divider()\n",
        "# Markdown testing\n",
        "st.markdown(\"# Markdown section\")\n",
        "st.markdown(\"[Picsums](https://picsum.photos/)\")\n",
        "st.markdown(\"\"\"\n",
        "    1. Static Random Image\n",
        "    2. Grayscale Image\"\"\")\n",
        "st.markdown(r\"$\\sqrt{ax+by+c}$\")\n",
        "\n",
        "st.divider()\n",
        "# LaTeX testing\n",
        "st.markdown(\"# LaTeX 1\")\n",
        "st.latex(r\"\\sqrt{a^2+b^2}\")\n",
        "st.latex(r\"\\frac{m}{n} = k\")\n",
        "st.latex(r\"\\sqrt[3]{z} = w\")\n",
        "\n",
        "st.divider()\n",
        "# Code block with syntax highlighting\n",
        "st.markdown(\"# Code block\")\n",
        "st.code(\"\"\"\n",
        "    import torch\n",
        "    import os\n",
        "    from google.colab import userdata\n",
        "\n",
        "    os.environ['NGROK_TOKEN'] = userdata.get('NGROK_TOKEN')\n",
        "    data = torch.Tensor([1, 2, 3])\n",
        "    print(data)\n",
        "\"\"\", language=\"python\")\n",
        "\n",
        "st.divider()\n",
        "# Write Block\n",
        "st.markdown(\"# Write\")\n",
        "st.write('Normal text')\n",
        "st.write('## Write Block Heading')\n",
        "st.write(r'$ \\sqrt{2x+2} $')\n",
        "st.write('1 + 1 = ', 2)\n",
        "\n",
        "st.divider()\n",
        "# Echo Block\n",
        "st.markdown(\"# Echo Block\")\n",
        "st.caption(\"Display code block on app, and execute it\")\n",
        "\n",
        "def get_mail_domain():\n",
        "    return 'gmail.com'\n",
        "with st.echo():\n",
        "    st.write('Displaying code in echo block')\n",
        "    def get_user_name():\n",
        "        return 'asimpleman'\n",
        "    user_name = get_user_name()\n",
        "    mail_domain = get_mail_domain()\n",
        "    st.write(user_name,'@', mail_domain)\n",
        "\n",
        "st.divider()\n",
        "st.markdown(\"# Media block\")\n",
        "st.image(\n",
        "    './image.jpg',\n",
        "    caption='picsums'\n",
        ")\n",
        "# st.audio('./audio.mp4')\n",
        "# st.video('./video.mp4')\n",
        "\n",
        "st.divider()\n",
        "st.markdown(\"# Widgets block\")\n",
        "name = st.text_input(\"What's your name\", \"James Francis Ryan\")\n",
        "st.write(\"Your name is\", name)\n",
        "\n",
        "st.divider()\n",
        "\n",
        "def on_agree_terms():\n",
        "    st.write(\"Thank for agree on Terms and Conditions\")\n",
        "agree = st.checkbox(\"Agree on Terms and Conditions\", on_change=on_agree_terms)\n",
        "if agree:\n",
        "    st.write(\"Great!\")\n",
        "\n",
        "form_color = st.radio(\n",
        "    \"Your favorite color:\",\n",
        "    ['Yellow', 'Bleu'],\n",
        "    captions = ['Vàng', 'Xanh']\n",
        ")\n",
        "st.write('form_radio', form_color)\n",
        "\n",
        "form_contact = st.selectbox(\n",
        "    \"Your contact:\",\n",
        "    (\"Email\", \"Home phone\", \"Mobile phone\"))\n",
        "\n",
        "st.write(\"Selected:\", form_contact)\n",
        "\n",
        "form_colors = st.multiselect(\n",
        "    \"Your favorite colors:\",\n",
        "    [\"Green\", \"Yellow\", \"Red\", \"Blue\"],\n",
        "    [\"Yellow\", \"Red\"])\n",
        "\n",
        "st.write(\"You selected:\", form_colors)\n",
        "\n",
        "form_color_slider = st.select_slider(\n",
        "    \"Your favorite color:\",\n",
        "    options=[\"red\", \"orange\", \"violet\"])\n",
        "\n",
        "st.write(\"My favorite color is\", form_color_slider)\n",
        "\n",
        "if st.button(\"Say hello\"):\n",
        "    st.write(\"Hello\")\n",
        "else:\n",
        "    st.write(\"Goodbye\")\n",
        "\n",
        "st.link_button(\n",
        "    \"Go to Google\",\n",
        "    \"https://www.google.com.vn/\")\n",
        "\n",
        "number = st.number_input(\"Insert a number\")\n",
        "st.write(\"The current number is \", number)\n",
        "\n",
        "values = st.slider(\n",
        "    \"Select a range of values\",\n",
        "    0.0, 100.0, (25.0, 75.0))\n",
        "st.write(\"Values:\", values)\n",
        "\n",
        "\n",
        "st.divider()\n",
        "st.markdown(\"# Uploader\")\n",
        "form_upload_files = st.file_uploader(\n",
        "    \"Select files\", accept_multiple_files=True)\n",
        "\n",
        "for uploaded_file in form_upload_files:\n",
        "    bytes_data = uploaded_file.read()\n",
        "    st.write(\"filename:\", uploaded_file.name)\n",
        "\n",
        "\n",
        "st.divider()\n",
        "st.markdown(\"# Form batching\")\n",
        "st.caption(\"Create a form that batches elements together with a Submit button\")\n",
        "with st.form(\"information_form\"):\n",
        "    col1, col2 = st.columns(2)\n",
        "    f_name = col1.text_input('First Name')\n",
        "    l_name = col2.text_input('Last Name')\n",
        "    submitted = st.form_submit_button(\"Submit\")\n",
        "    if submitted:\n",
        "        st.write(\"First Name: \", f_name, \" - Last Name:\", l_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5c7W3aM3qgf",
        "outputId": "7771a1dd-21c6-41e5-8394-46c7f93db078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.245.135.162:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠸\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run streamlit-components.py & npx ngrok http 8501 --log=stdout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICYb8Afa9Uy-"
      },
      "source": [
        "## **Applications**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__wI9ciEXklL"
      },
      "source": [
        "### **Sentiment Analysis using NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PKynfjQLQUiH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61802c78-b19b-482d-9c8c-3539b0e75b91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sentiment_analysis.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile sentiment_analysis.py\n",
        "\n",
        "import streamlit as st\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "def main():\n",
        "    st.title('Sentiment Analysis using NLTK lib')\n",
        "    st.title('Using vader_lexicon')\n",
        "    sentence = st.text_input(\"Sentence: \", \"This movie is bad.\")\n",
        "\n",
        "    sentiment_scores = sid.polarity_scores(sentence)\n",
        "    st.write(f'Sentiment Scores: ', sentiment_scores)\n",
        "\n",
        "    # Sort score DESC.\n",
        "    sentiment_scores = sorted(\n",
        "        sentiment_scores.items(),\n",
        "        key=lambda item: item[1],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    if sentiment_scores[0][0] == 'neg':\n",
        "        st.success(f'Sentiment: Negative')\n",
        "    else:\n",
        "        st.success(f'Sentiment: Positive')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "     main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SnfB3TPnXqW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91329a8f-b2f9-445b-ebbc-90f5d41031f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.245.135.162:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K\u001b[32mINFO\u001b[0m[01-04|15:00:43] no configuration paths supplied \n",
            "\u001b[32mINFO\u001b[0m[01-04|15:00:43] using configuration at default config path \u001b[32mpath\u001b[0m=/root/.config/ngrok/ngrok.yml\n",
            "\u001b[32mINFO\u001b[0m[01-04|15:00:43] open config file                         \u001b[32mpath\u001b[0m=/root/.config/ngrok/ngrok.yml \u001b[32merr\u001b[0m=nil\n",
            "t=2025-01-04T15:00:43+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "t=2025-01-04T15:00:43+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "t=2025-01-04T15:00:43+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "t=2025-01-04T15:00:43+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=command_line addr=http://localhost:8501 url=https://dfc4-35-245-135-162.ngrok-free.app\n",
            "t=2025-01-04T15:00:46+0000 lvl=info msg=\"join connections\" obj=join id=dc3d978c73ae l=127.0.0.1:8501 r=115.75.32.98:59049\n",
            "t=2025-01-04T15:00:46+0000 lvl=info msg=\"join connections\" obj=join id=a295381528e4 l=127.0.0.1:8501 r=115.75.32.98:59049\n",
            "t=2025-01-04T15:00:46+0000 lvl=info msg=\"join connections\" obj=join id=3e6c49d2fe2b l=127.0.0.1:8501 r=115.75.32.98:59049\n",
            "t=2025-01-04T15:00:46+0000 lvl=info msg=\"join connections\" obj=join id=76a0f85d3d08 l=127.0.0.1:8501 r=115.75.32.98:59049\n",
            "t=2025-01-04T15:00:46+0000 lvl=info msg=\"join connections\" obj=join id=1d3a5cc940a9 l=127.0.0.1:8501 r=115.75.32.98:59049\n",
            "t=2025-01-04T15:00:55+0000 lvl=info msg=\"join connections\" obj=join id=c094aa8862df l=127.0.0.1:8501 r=115.75.32.98:59350\n",
            "t=2025-01-04T15:00:55+0000 lvl=info msg=\"join connections\" obj=join id=969b40e46d9a l=127.0.0.1:8501 r=115.75.32.98:59350\n",
            "t=2025-01-04T15:00:56+0000 lvl=info msg=\"join connections\" obj=join id=4a3139387a38 l=127.0.0.1:8501 r=115.75.32.98:59351\n",
            "t=2025-01-04T15:00:57+0000 lvl=info msg=\"join connections\" obj=join id=6d0dc3132949 l=127.0.0.1:8501 r=115.75.32.98:59429\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "t=2025-01-04T15:01:01+0000 lvl=info msg=\"join connections\" obj=join id=8f047e2e533b l=127.0.0.1:8501 r=115.75.32.98:59429\n",
            "t=2025-01-04T15:01:01+0000 lvl=info msg=\"join connections\" obj=join id=0d929222bce5 l=127.0.0.1:8501 r=115.75.32.98:59429\n",
            "t=2025-01-04T15:01:01+0000 lvl=info msg=\"join connections\" obj=join id=e49c66ef642a l=127.0.0.1:8501 r=115.75.32.98:59429\n",
            "t=2025-01-04T15:01:01+0000 lvl=info msg=\"join connections\" obj=join id=4ffb3e305d6f l=127.0.0.1:8501 r=115.75.32.98:59429\n",
            "t=2025-01-04T15:01:01+0000 lvl=info msg=\"join connections\" obj=join id=d3aabef7e875 l=127.0.0.1:8501 r=115.75.32.98:59429\n",
            "t=2025-01-04T15:01:01+0000 lvl=info msg=\"join connections\" obj=join id=34cc2c81e398 l=127.0.0.1:8501 r=115.75.32.98:59429\n",
            "t=2025-01-04T15:01:01+0000 lvl=info msg=\"join connections\" obj=join id=8160126d227e l=127.0.0.1:8501 r=115.75.32.98:59429\n",
            "t=2025-01-04T15:01:01+0000 lvl=info msg=\"join connections\" obj=join id=0c10fd21c818 l=127.0.0.1:8501 r=115.75.32.98:59429\n",
            "t=2025-01-04T15:01:01+0000 lvl=info msg=\"join connections\" obj=join id=ae31ac1c63d3 l=127.0.0.1:8501 r=115.75.32.98:59429\n",
            "t=2025-01-04T15:01:01+0000 lvl=info msg=\"join connections\" obj=join id=d5b7a5fff2fc l=127.0.0.1:8501 r=115.75.32.98:59429\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "t=2025-01-04T15:03:09+0000 lvl=info msg=\"received stop request\" obj=app stopReq=\"{err:<nil> restart:false}\"\n",
            "t=2025-01-04T15:03:09+0000 lvl=info msg=\"session closing\" obj=tunnels.session err=nil\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run sentiment_analysis.py & npx ngrok http 8501 --log=stdout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiGJngoDFXUq"
      },
      "source": [
        "### **Contextual Spell Correction using Spacy and contextualSpellCheck**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "uGsuWhA1X0uq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20f283c-2aaf-4a3d-f08d-53c9a3c62171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/128.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.1/128.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/282.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.6/282.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q spacy contextualSpellCheck==0.4.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "v8zpCDr_EKQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf87208-4a5d-4f7b-eac1-a36a6ce6ac96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting spell_corrector.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile spell_corrector.py\n",
        "import streamlit as st\n",
        "import spacy\n",
        "import contextualSpellCheck\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "contextualSpellCheck.add_to_pipe(nlp)\n",
        "\n",
        "def spell_corrector(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    st.write('doc._', doc, doc._, doc._.performed_spellCheck, doc._.outcome_spellCheck)\n",
        "\n",
        "    return doc._.performed_spellCheck, doc._.outcome_spellCheck\n",
        "\n",
        "def main():\n",
        "    st.title('Contextual Spell Correction')\n",
        "    st.title('Spacy + contextualSpellCheck')\n",
        "    sentence = st.text_input(\"Sentence: \", \"I go to slaep.\")\n",
        "    result = spell_corrector(sentence)\n",
        "\n",
        "    if result[0]:\n",
        "        st.success(f'Corrected: {result[1]}')\n",
        "    else:\n",
        "        st.success(f'Sentence: {sentence}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "     main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tfxIh-GMX8f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8157401-5246-4556-d2b8-f6c9f41fe909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.245.135.162:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\u001b[32mINFO\u001b[0m[01-04|15:19:27] no configuration paths supplied \n",
            "\u001b[32mINFO\u001b[0m[01-04|15:19:27] using configuration at default config path \u001b[32mpath\u001b[0m=/root/.config/ngrok/ngrok.yml\n",
            "\u001b[32mINFO\u001b[0m[01-04|15:19:27] open config file                         \u001b[32mpath\u001b[0m=/root/.config/ngrok/ngrok.yml \u001b[32merr\u001b[0m=nil\n",
            "t=2025-01-04T15:19:27+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "t=2025-01-04T15:19:27+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "t=2025-01-04T15:19:27+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "t=2025-01-04T15:19:27+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=command_line addr=http://localhost:8501 url=https://1782-35-245-135-162.ngrok-free.app\n",
            "t=2025-01-04T15:19:50+0000 lvl=info msg=\"join connections\" obj=join id=3a18b4422cbe l=127.0.0.1:8501 r=115.75.32.98:61117\n",
            "t=2025-01-04T15:19:51+0000 lvl=info msg=\"join connections\" obj=join id=c3fd481adcae l=127.0.0.1:8501 r=115.75.32.98:61117\n",
            "t=2025-01-04T15:19:51+0000 lvl=info msg=\"join connections\" obj=join id=0c5259e8cf2e l=127.0.0.1:8501 r=115.75.32.98:61117\n",
            "t=2025-01-04T15:19:51+0000 lvl=info msg=\"join connections\" obj=join id=a51c0d76f583 l=127.0.0.1:8501 r=115.75.32.98:61117\n",
            "t=2025-01-04T15:19:52+0000 lvl=info msg=\"join connections\" obj=join id=5c73938f8863 l=127.0.0.1:8501 r=115.75.32.98:61117\n",
            "t=2025-01-04T15:20:11+0000 lvl=info msg=\"join connections\" obj=join id=052b95a4ce91 l=127.0.0.1:8501 r=115.75.32.98:61803\n",
            "t=2025-01-04T15:20:11+0000 lvl=info msg=\"join connections\" obj=join id=54dfb703d04c l=127.0.0.1:8501 r=115.75.32.98:61803\n",
            "t=2025-01-04T15:20:13+0000 lvl=info msg=\"join connections\" obj=join id=6f9b051947e8 l=127.0.0.1:8501 r=115.75.32.98:61877\n",
            "2025-01-04 15:20:22.434977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-04 15:20:22.484580: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-04 15:20:22.498263: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-04 15:20:22.530467: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-04 15:20:24.667389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "t=2025-01-04T15:20:26+0000 lvl=info msg=\"join connections\" obj=join id=3c588673a3f2 l=127.0.0.1:8501 r=115.75.32.98:62324\n",
            "t=2025-01-04T15:20:26+0000 lvl=info msg=\"join connections\" obj=join id=e1a59b18c3fd l=127.0.0.1:8501 r=115.75.32.98:62324\n",
            "t=2025-01-04T15:20:26+0000 lvl=info msg=\"join connections\" obj=join id=2232c7c3b17b l=127.0.0.1:8501 r=115.75.32.98:62324\n",
            "t=2025-01-04T15:20:26+0000 lvl=info msg=\"join connections\" obj=join id=70de6a6b70ae l=127.0.0.1:8501 r=115.75.32.98:62324\n",
            "t=2025-01-04T15:20:26+0000 lvl=info msg=\"join connections\" obj=join id=2cffc6e807f2 l=127.0.0.1:8501 r=115.75.32.98:62324\n",
            "t=2025-01-04T15:20:26+0000 lvl=info msg=\"join connections\" obj=join id=76ea76fc79c5 l=127.0.0.1:8501 r=115.75.32.98:62324\n",
            "t=2025-01-04T15:20:26+0000 lvl=info msg=\"join connections\" obj=join id=92da118c9ebd l=127.0.0.1:8501 r=115.75.32.98:62324\n",
            "t=2025-01-04T15:20:26+0000 lvl=info msg=\"join connections\" obj=join id=f7a7c52d9c1e l=127.0.0.1:8501 r=115.75.32.98:62324\n",
            "t=2025-01-04T15:20:26+0000 lvl=info msg=\"join connections\" obj=join id=6e9847cd1523 l=127.0.0.1:8501 r=115.75.32.98:62324\n",
            "t=2025-01-04T15:20:26+0000 lvl=info msg=\"join connections\" obj=join id=8274e01a5cb2 l=127.0.0.1:8501 r=115.75.32.98:62324\n",
            "2025-01-04 15:20:27.167 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "t=2025-01-04T15:22:00+0000 lvl=info msg=\"received stop request\" obj=app stopReq=\"{err:<nil> restart:false}\"\n",
            "t=2025-01-04T15:22:00+0000 lvl=info msg=\"session closing\" obj=tunnels.session err=nil\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run spell_corrector.py & npx ngrok http 8501 --log=stdout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M9EXKzGFOTa"
      },
      "source": [
        "### **English-Vietnamese Machine Translation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "kOR1GtkE9XHW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "849df3cc-15ac-4990-a94a-d4c75c6941b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.47.1 in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.1) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.1) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.1) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.1) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.47.1) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.47.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.47.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.47.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.47.1) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.47.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6L4-RrbX9vWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af071e2d-6939-48ae-e11e-cee363d08920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing en_vi_machine_translation.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile en_vi_machine_translation.py\n",
        "import streamlit as st\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# Check and use better device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# download model\n",
        "model_name = \"thainq107/t5-small-en-vi\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Change to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "def translate(sentence, tokenizer, model):\n",
        "    tokenized_sentence = tokenizer.encode(sentence, return_tensors=\"pt\").to(device)\n",
        "    output_ids = model.generate(tokenized_sentence, max_length=128)\n",
        "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return output\n",
        "\n",
        "def main():\n",
        "    st.title('English-Vietnamese Machine Translation')\n",
        "    st.title('Model: T5. Dataset: En-Vi')\n",
        "    sentence = st.text_input(\"Sentence: \", \"I go to school.\")\n",
        "    result = translate(sentence, tokenizer, model)\n",
        "    st.success(f'Translated: {result}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "     main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "c1gW8jGg_uFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e89ca194-0bb5-446f-dae9-fc05acc4945e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.245.135.162:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K\u001b[32mINFO\u001b[0m[01-04|15:27:29] no configuration paths supplied \n",
            "\u001b[32mINFO\u001b[0m[01-04|15:27:29] using configuration at default config path \u001b[32mpath\u001b[0m=/root/.config/ngrok/ngrok.yml\n",
            "\u001b[32mINFO\u001b[0m[01-04|15:27:29] open config file                         \u001b[32mpath\u001b[0m=/root/.config/ngrok/ngrok.yml \u001b[32merr\u001b[0m=nil\n",
            "t=2025-01-04T15:27:29+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "t=2025-01-04T15:27:29+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "t=2025-01-04T15:27:29+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "t=2025-01-04T15:27:29+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=command_line addr=http://localhost:8501 url=https://8187-35-245-135-162.ngrok-free.app\n",
            "t=2025-01-04T15:27:32+0000 lvl=info msg=\"join connections\" obj=join id=e818691aea0a l=127.0.0.1:8501 r=115.75.32.98:60217\n",
            "t=2025-01-04T15:27:33+0000 lvl=info msg=\"join connections\" obj=join id=cb0c25a8944c l=127.0.0.1:8501 r=115.75.32.98:60217\n",
            "t=2025-01-04T15:27:33+0000 lvl=info msg=\"join connections\" obj=join id=a76ec972e50c l=127.0.0.1:8501 r=115.75.32.98:60217\n",
            "t=2025-01-04T15:27:33+0000 lvl=info msg=\"join connections\" obj=join id=53666e56c133 l=127.0.0.1:8501 r=115.75.32.98:60217\n",
            "t=2025-01-04T15:27:33+0000 lvl=info msg=\"join connections\" obj=join id=d035f510c3fd l=127.0.0.1:8501 r=115.75.32.98:60217\n",
            "t=2025-01-04T15:27:42+0000 lvl=info msg=\"join connections\" obj=join id=46c0a9072d3a l=127.0.0.1:8501 r=115.75.32.98:60536\n",
            "tokenizer_config.json: 100% 892/892 [00:00<00:00, 4.12MB/s]\n",
            "spiece.model: 100% 4.31M/4.31M [00:00<00:00, 15.9MB/s]\n",
            "special_tokens_map.json: 100% 416/416 [00:00<00:00, 2.00MB/s]\n",
            "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "config.json: 100% 806/806 [00:00<00:00, 4.42MB/s]\n",
            "2025-01-04 15:27:52.869752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-04 15:27:52.889518: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-04 15:27:52.895243: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-04 15:27:52.909954: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-04 15:27:54.434013: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "model.safetensors: 100% 1.20G/1.20G [00:29<00:00, 40.4MB/s]\n",
            "generation_config.json: 100% 142/142 [00:00<00:00, 700kB/s]\n",
            "2025-01-04 15:28:30.985 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "t=2025-01-04T15:28:31+0000 lvl=info msg=\"join connections\" obj=join id=4342dd3d44e0 l=127.0.0.1:8501 r=115.75.32.98:62184\n",
            "t=2025-01-04T15:28:31+0000 lvl=info msg=\"join connections\" obj=join id=6d0ddd088da5 l=127.0.0.1:8501 r=115.75.32.98:62184\n",
            "t=2025-01-04T15:28:31+0000 lvl=info msg=\"join connections\" obj=join id=ce5680f1c978 l=127.0.0.1:8501 r=115.75.32.98:62184\n",
            "t=2025-01-04T15:28:31+0000 lvl=info msg=\"join connections\" obj=join id=8319c1dedea7 l=127.0.0.1:8501 r=115.75.32.98:62184\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "t=2025-01-04T15:31:55+0000 lvl=info msg=\"received stop request\" obj=app stopReq=\"{err:<nil> restart:false}\"\n",
            "t=2025-01-04T15:31:55+0000 lvl=info msg=\"session closing\" obj=tunnels.session err=nil\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run en_vi_machine_translation.py & npx ngrok http 8501 --log=stdout"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}